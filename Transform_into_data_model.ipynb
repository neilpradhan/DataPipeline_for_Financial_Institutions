{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "accounts_df = pd.read_excel(\"Processed_file\\Cleaned_data\\cleaned_accounts.xlsx\", sheet_name='Sheet1')\n",
    "deposits_df = pd.read_excel(\"Processed_file\\Cleaned_data\\cleaned_deposits.xlsx\", sheet_name='Sheet1')\n",
    "loans_df = pd.read_excel(\"Processed_file\\Cleaned_data\\cleaned_loans.xlsx\", sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts_df.rename(columns={'amount': 'account_amount'}, inplace=True)\n",
    "deposits_df.rename(columns={'amount': 'deposit_amount'}, inplace=True)\n",
    "loans_df.rename(columns={'amount': 'loan_amount'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dimension Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer_Dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unique_customers_deposits = deposits_df[['customer', 'customer_type']].drop_duplicates()\n",
    "unique_customers_loans = loans_df[['customer', 'customer_type']].drop_duplicates()\n",
    "\n",
    "# Combine the unique customers and remove duplicates\n",
    "all_unique_customers = pd.concat([unique_customers_deposits, unique_customers_loans]).drop_duplicates()\n",
    "\n",
    "# Create a DataFrame for unique customers with customer_key\n",
    "unique_customers_df = all_unique_customers.copy()\n",
    "unique_customers_df['customer_key'] = range(1, len(unique_customers_df) + 1)\n",
    "\n",
    "# Display the first few rows of the unique customers DataFrame\n",
    "# print(unique_customers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country_Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country  country_key\n",
      "0      FI            1\n",
      "1      NO            2\n",
      "2      SE            3\n"
     ]
    }
   ],
   "source": [
    "unique_countries_deposits = deposits_df['country'].unique()\n",
    "unique_countries_loans = loans_df['country'].unique()\n",
    "\n",
    "# Combine the unique countries and remove duplicates\n",
    "all_unique_countries = np.unique(np.concatenate((unique_countries_deposits, unique_countries_loans)))\n",
    "\n",
    "# Create a DataFrame for unique countries\n",
    "unique_countries_df = pd.DataFrame(all_unique_countries, columns=['country'])\n",
    "unique_countries_df['country_key'] = range(1, len(unique_countries_df) + 1)\n",
    "\n",
    "# Display the first few rows of the unique countries DataFrame\n",
    "print(unique_countries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Currency_Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  currency  currency_key\n",
      "0      EUR             1\n",
      "1      NOK             2\n",
      "2      SEK             3\n"
     ]
    }
   ],
   "source": [
    "unique_currencies_deposits = deposits_df['currency'].unique()\n",
    "unique_currencies_loans = loans_df['currency'].unique()\n",
    "\n",
    "# Combine the unique currencies and remove duplicates\n",
    "all_unique_currencies = np.unique(np.concatenate((unique_currencies_deposits, unique_currencies_loans)))\n",
    "\n",
    "# Create a DataFrame for unique currencies\n",
    "unique_currencies_df = pd.DataFrame(all_unique_currencies, columns=['currency'])\n",
    "unique_currencies_df['currency_key'] = range(1, len(unique_currencies_df) + 1)\n",
    "\n",
    "# Display the first few rows of the unique currencies DataFrame\n",
    "print(unique_currencies_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accounts_Name_Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    account_name  account_name_key\n",
      "0  Unsecured Personal Loan - EUR                 1\n",
      "1              Credit Card - NOK                 2\n",
      "2        Corporate Leasing - SEK                 3\n",
      "3  Unsecured Personal Loan - SEK                 4\n",
      "4                    Fixed - EUR                 5\n",
      "5                 Floating - EUR                 6\n",
      "6                    Fixed - SEK                 7\n",
      "7                 Floating - SEK                 8\n"
     ]
    }
   ],
   "source": [
    "unique_account_names = pd.DataFrame(accounts_df['account_name'].unique(), columns=['account_name'])\n",
    "unique_account_names['account_name_key'] = range(1, len(unique_account_names) + 1)\n",
    "\n",
    "# Display the account name dimension DataFrame\n",
    "print(unique_account_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date_Dimension\n",
    "\n",
    "1. First we find the minimum and maximum date in data (Can be optimized for scaling) \n",
    "\n",
    "2. Create Date Dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Min Date: 2019-12-01 00:00:00\n",
      "Overall Max Date: 2035-12-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Convert relevant date columns to datetime\n",
    "accounts_df['reference_date'] = pd.to_datetime(accounts_df['reference_date'])\n",
    "deposits_df['start_date'] = pd.to_datetime(deposits_df['start_date'])\n",
    "deposits_df['maturity_date'] = pd.to_datetime(deposits_df['maturity_date'])\n",
    "deposits_df['reference_date'] = pd.to_datetime(deposits_df['reference_date'])\n",
    "loans_df['start_date'] = pd.to_datetime(loans_df['start_date'])\n",
    "loans_df['maturity_date'] = pd.to_datetime(loans_df['maturity_date'])\n",
    "loans_df['reference_date'] = pd.to_datetime(loans_df['reference_date'])\n",
    "\n",
    "# Find the min and max dates\n",
    "min_date_accounts = accounts_df['reference_date'].min()\n",
    "max_date_accounts = accounts_df['reference_date'].max()\n",
    "\n",
    "min_date_deposits = deposits_df[['start_date', 'maturity_date', 'reference_date']].min().min()\n",
    "max_date_deposits = deposits_df[['start_date', 'maturity_date', 'reference_date']].max().max()\n",
    "\n",
    "min_date_loans = loans_df[['start_date', 'maturity_date', 'reference_date']].min().min()\n",
    "max_date_loans = loans_df[['start_date', 'maturity_date', 'reference_date']].max().max()\n",
    "\n",
    "# Overall min and max dates\n",
    "overall_min_date = min(min_date_accounts, min_date_deposits, min_date_loans)\n",
    "overall_max_date = max(max_date_accounts, max_date_deposits, max_date_loans)\n",
    "\n",
    "print(f\"Overall Min Date: {overall_min_date}\")\n",
    "print(f\"Overall Max Date: {overall_max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  date_key  year  month  day  quarter  day_of_week   day_name  \\\n",
      "0 2019-12-01  20191201  2019     12    1        4            6     Sunday   \n",
      "1 2019-12-02  20191202  2019     12    2        4            0     Monday   \n",
      "2 2019-12-03  20191203  2019     12    3        4            1    Tuesday   \n",
      "3 2019-12-04  20191204  2019     12    4        4            2  Wednesday   \n",
      "4 2019-12-05  20191205  2019     12    5        4            3   Thursday   \n",
      "\n",
      "  month_name  is_weekend  \n",
      "0   December        True  \n",
      "1   December       False  \n",
      "2   December       False  \n",
      "3   December       False  \n",
      "4   December       False  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the date range for the date dimension based on overall min and max dates\n",
    "date_range = pd.date_range(start='2019-12-01', end='2035-12-05')\n",
    "\n",
    "# Create a DataFrame for the date dimension\n",
    "date_dimension = pd.DataFrame(date_range, columns=['date'])\n",
    "date_dimension['date_key'] = date_dimension['date'].dt.strftime('%Y%m%d').astype(int)\n",
    "date_dimension['year'] = date_dimension['date'].dt.year\n",
    "date_dimension['month'] = date_dimension['date'].dt.month\n",
    "date_dimension['day'] = date_dimension['date'].dt.day\n",
    "date_dimension['quarter'] = date_dimension['date'].dt.quarter\n",
    "date_dimension['day_of_week'] = date_dimension['date'].dt.dayofweek\n",
    "date_dimension['day_name'] = date_dimension['date'].dt.day_name()\n",
    "date_dimension['month_name'] = date_dimension['date'].dt.month_name()\n",
    "date_dimension['is_weekend'] = date_dimension['date'].dt.dayofweek >= 5\n",
    "\n",
    "# Display the first few rows of the date dimension DataFrame\n",
    "print(date_dimension.head())\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loan_type_Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_type_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unsecured Personal Loan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Corporate Leasing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 loan_type  loan_type_key\n",
       "0  Unsecured Personal Loan              1\n",
       "1              Credit Card              2\n",
       "2        Corporate Leasing              3"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_types = loans_df['loan_type'].unique()\n",
    "loan_type_df = pd.DataFrame(loan_types, columns=['loan_type'])\n",
    "loan_type_df['loan_type_key'] = range(1, len(loan_type_df) + 1)\n",
    "loan_type_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deposit_type_Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deposit_type</th>\n",
       "      <th>deposit_type_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fixed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  deposit_type  deposit_type_key\n",
       "0        Fixed                 1"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deposit_types = deposits_df['deposit_type'].unique()\n",
    "deposit_type_df = pd.DataFrame(deposit_types, columns=['deposit_type'])\n",
    "deposit_type_df['deposit_type_key'] = range(1, len(deposit_type_df) + 1)\n",
    "deposit_type_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_accounts = accounts_df.copy()\n",
    "fact_deposits = deposits_df.copy()\n",
    "fact_loans = loans_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Surrogate Primary Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_accounts['account_surr_primarykey'] = range(1, len(fact_accounts) + 1)\n",
    "fact_deposits['deposits_surr_primarykey'] = range(1, len(fact_deposits)+1)\n",
    "fact_loans['loans_surr_primarykey'] = range(1, len(fact_loans)+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maping date_dimension to Fact_tables\n",
    "1. Map all the dates with the key \n",
    "2. Drop the original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map dates to date keys\n",
    "def map_date_to_key(df, date_column):\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df[date_column + '_key'] = df[date_column].dt.strftime('%Y%m%d').astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Add date keys to fact_accounts\n",
    "fact_accounts = map_date_to_key(fact_accounts, 'reference_date')\n",
    "\n",
    "# Add date keys to fact_deposits\n",
    "fact_deposits = map_date_to_key(fact_deposits, 'start_date')\n",
    "fact_deposits = map_date_to_key(fact_deposits, 'maturity_date')\n",
    "fact_deposits = map_date_to_key(fact_deposits, 'reference_date')\n",
    "\n",
    "# Add date keys to fact_loans\n",
    "fact_loans = map_date_to_key(fact_loans, 'start_date')\n",
    "fact_loans = map_date_to_key(fact_loans, 'maturity_date')\n",
    "fact_loans = map_date_to_key(fact_loans, 'reference_date')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_deposits.drop(columns = ['start_date','maturity_date','reference_date'], inplace=True)\n",
    "fact_loans.drop(columns = ['start_date','maturity_date','reference_date'], inplace=True)\n",
    "fact_accounts.drop(columns=['reference_date'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maping cutomer_dimension to fact_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_customer_to_key(df, unique_customers_df):\n",
    "    df = df.merge(unique_customers_df[['customer', 'customer_key']], on='customer', how='left')\n",
    "    # df.drop(columns=['customer'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Add customer_key to fact_deposits and drop the original customer column\n",
    "fact_deposits = map_customer_to_key(fact_deposits, unique_customers_df)\n",
    "\n",
    "# Add customer_key to fact_loans and drop the original customer column\n",
    "fact_loans = map_customer_to_key(fact_loans, unique_customers_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_deposits.drop(columns=['customer','customer_type'], inplace=True)\n",
    "fact_loans.drop(columns=['customer','customer_type'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maping account_name dimensions to fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_account_name_to_key(df, account_name_df):\n",
    "    df = df.merge(account_name_df, on='account_name', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "fact_accounts = map_account_name_to_key(fact_accounts, unique_account_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_accounts.drop(columns=['account_name'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maping Currency dimension to  fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_currency_to_key(df, unique_currencies_df):\n",
    "    df = df.merge(unique_currencies_df, on='currency', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "# Add currency_key to fact_deposits and drop the original currency column\n",
    "fact_deposits = map_currency_to_key(fact_deposits, unique_currencies_df)\n",
    "\n",
    "# Add currency_key to fact_loans and drop the original currency column\n",
    "fact_loans = map_currency_to_key(fact_loans, unique_currencies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_deposits.drop(columns=['currency'], inplace= True)\n",
    "fact_loans.drop(columns=['currency'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Country dimension to Fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_country_to_key(df, unique_countries_df):\n",
    "    df = df.merge(unique_countries_df, on='country', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Add country_key to fact_deposits and drop the original country column\n",
    "fact_deposits = map_country_to_key(fact_deposits, unique_countries_df)\n",
    "\n",
    "# Add country_key to fact_loans and drop the original country column\n",
    "fact_loans = map_country_to_key(fact_loans, unique_countries_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_deposits.drop(columns=['country'], inplace= True)\n",
    "fact_loans.drop(columns=['country'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Deposit_type dimensions to  Fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_deposit_type_to_key(df, deposit_type_df):\n",
    "    df = df.merge(deposit_type_df, on='deposit_type', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "# Add deposit_type_key to fact_deposits and drop the original deposit_type column\n",
    "fact_deposits = map_deposit_type_to_key(fact_deposits, deposit_type_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_deposits.drop(columns=['deposit_type'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Loan_type dimensions to Fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_loan_type_to_key(df, loan_type_df):\n",
    "    df = df.merge(loan_type_df, on='loan_type', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Add loan_type_key to fact_loans and drop the original loan_type column\n",
    "fact_loans = map_loan_type_to_key(fact_loans, loan_type_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_loans.drop(columns=['loan_type'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data to Data warehouse\n",
    "\n",
    "For the purpose of this assignment I have uploaded the files in a folder called Data warehouse but I can also upload them on Google Big Query using python package\n",
    "\n",
    "\n",
    "1. **Dimension Table** Add three columns 'start_date', 'end_date' and 'active_flag' for Slowly changing dimensions type 2 and then load the data into data warehouse\n",
    "\n",
    "2. **Fact Tables**  Ensure surrogate Primary key for Fact tables exist and then load "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Function to add SCD2 columns\n",
    "def add_scd2_columns(df):\n",
    "    df['start_date'] = datetime.now()\n",
    "    df['end_date'] = pd.to_datetime('2262-04-11')\n",
    "    df['active_flag'] = 'Y'\n",
    "    return df\n",
    "\n",
    "os.makedirs('Data_warehouse', exist_ok=True)\n",
    "\n",
    "# Add SCD2 columns to each dimension table\n",
    "unique_customers_df = add_scd2_columns(unique_customers_df)\n",
    "unique_countries_df = add_scd2_columns(unique_countries_df)\n",
    "unique_currencies_df = add_scd2_columns(unique_currencies_df)\n",
    "unique_account_names = add_scd2_columns(unique_account_names)\n",
    "date_dimension = add_scd2_columns(date_dimension)\n",
    "loan_type_df = add_scd2_columns(loan_type_df)\n",
    "deposit_type_df = add_scd2_columns(deposit_type_df)\n",
    "\n",
    "\n",
    "\n",
    "# Function to save DataFrame to CSV\n",
    "def save_to_csv(df, file_name):\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# Save each updated DataFrame to CSV\n",
    "save_to_csv(unique_customers_df, 'Data_warehouse/unique_customers.csv')\n",
    "save_to_csv(unique_countries_df, 'Data_warehouse/unique_countries.csv')\n",
    "save_to_csv(unique_currencies_df, 'Data_warehouse/unique_currencies.csv')\n",
    "save_to_csv(unique_account_names, 'Data_warehouse/unique_account_names.csv')\n",
    "save_to_csv(date_dimension, 'Data_warehouse/date_dimension.csv')\n",
    "save_to_csv(loan_type_df, 'Data_warehouse/loan_type.csv')\n",
    "save_to_csv(deposit_type_df, 'Data_warehouse/deposit_type.csv')\n",
    "\n",
    "# The files are now ready to be loaded into the data warehouse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_to_csv(fact_deposits, 'Data_warehouse/fact_deposits.csv')\n",
    "save_to_csv(fact_loans, 'Data_warehouse/fact_loans.csv')\n",
    "save_to_csv(fact_accounts, 'Data_warehouse/fact_accounts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the max surrogate keys for Fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_name</th>\n",
       "      <th>max_deposits_surr_primarykey</th>\n",
       "      <th>max_account_surr_primarykey</th>\n",
       "      <th>max_surr_primarykey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fact_deposits</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fact_accounts</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fact_loans</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      table_name  max_deposits_surr_primarykey  max_account_surr_primarykey  \\\n",
       "0  fact_deposits                            90                            0   \n",
       "1  fact_accounts                             0                           16   \n",
       "2     fact_loans                             0                            0   \n",
       "\n",
       "   max_surr_primarykey  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                  300  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create the Data_warehouse folder if it doesn't exist\n",
    "os.makedirs('Data_warehouse', exist_ok=True)\n",
    "\n",
    "max_deposits_key = fact_deposits['deposits_surr_primarykey'].max()\n",
    "max_accounts_key = fact_accounts['account_surr_primarykey'].max()\n",
    "max_loans_key = fact_loans['loans_surr_primarykey'].max()\n",
    "\n",
    "\n",
    "\n",
    "max_keys_data = [\n",
    "    {\"table_name\": \"fact_deposits\", \"max_deposits_surr_primarykey\": max_deposits_key},\n",
    "    {\"table_name\": \"fact_accounts\", \"max_account_surr_primarykey\": max_accounts_key},\n",
    "    {\"table_name\": \"fact_loans\", \"max_surr_primarykey\": max_loans_key}\n",
    "]\n",
    "max_keys_df = pd.DataFrame(max_keys_data)\n",
    "\n",
    "max_keys_df['max_deposits_surr_primarykey'] = max_keys_df['max_deposits_surr_primarykey'].fillna(0).astype(int)\n",
    "max_keys_df['max_account_surr_primarykey'] = max_keys_df['max_account_surr_primarykey'].fillna(0).astype(int)\n",
    "max_keys_df['max_surr_primarykey'] = max_keys_df['max_surr_primarykey'].fillna(0).astype(int)\n",
    "\n",
    "# Save the max keys data to a CSV file\n",
    "max_keys_df.to_csv('Data_warehouse/max_keys.csv', index=False)\n",
    "\n",
    "# print(\"Max keys data saved to Data_warehouse/max_surr_keys.csv\")\n",
    "max_keys_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Big Query Data warehouse upload files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_20216\\745117729.py:10: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists='replace', credentials=credentials)\n",
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_20216\\745117729.py:10: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists='replace', credentials=credentials)\n",
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_20216\\745117729.py:10: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists='replace', credentials=credentials)\n",
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_20216\\745117729.py:10: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists='replace', credentials=credentials)\n",
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_20216\\745117729.py:10: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists='replace', credentials=credentials)\n",
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_20216\\745117729.py:10: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists='replace', credentials=credentials)\n",
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_20216\\745117729.py:10: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists='replace', credentials=credentials)\n",
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_20216\\745117729.py:10: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists='replace', credentials=credentials)\n",
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_20216\\745117729.py:10: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists='replace', credentials=credentials)\n",
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_20216\\745117729.py:10: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists='replace', credentials=credentials)\n",
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_20216\\745117729.py:10: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists='replace', credentials=credentials)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Set the environment variable for authentication\n",
    "key_path = \"C:/Users/epranei/Downloads/calm-cove-423918-t0-ce8d5f6922f1.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "def save_to_bigquery(df, table_id):\n",
    "    df.to_gbq(table_id, project_id=project_id, if_exists='replace', credentials=credentials)\n",
    "\n",
    "# Define your project and dataset ID\n",
    "project_id = 'calm-cove-423918-t0'\n",
    "dataset_id = 'Advisense'\n",
    "\n",
    "# DataFrames to be saved to BigQuery\n",
    "dataframes = {\n",
    "    'unique_customers': unique_customers_df,\n",
    "    'unique_countries': unique_countries_df,\n",
    "    'unique_currencies': unique_currencies_df,\n",
    "    'unique_account_names': unique_account_names,\n",
    "    'date_dimension': date_dimension,\n",
    "    'loan_type': loan_type_df,\n",
    "    'deposit_type': deposit_type_df,\n",
    "    'fact_deposits': fact_deposits,\n",
    "    'fact_loans': fact_loans,\n",
    "    'fact_accounts': fact_accounts\n",
    "}\n",
    "\n",
    "# Save each DataFrame to a BigQuery table\n",
    "for table_name, df in dataframes.items():\n",
    "    table_id = f'{dataset_id}.{table_name}'\n",
    "    save_to_bigquery(df, table_id)\n",
    "\n",
    "# Save max_keys_df to BigQuery\n",
    "max_keys_table_id = f'{dataset_id}.max_keys'\n",
    "save_to_bigquery(max_keys_df, max_keys_table_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advicense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

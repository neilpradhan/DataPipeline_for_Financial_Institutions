{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Incremental Data\n",
    "\n",
    "Assuming that we have already done the first load and our data is in the datawarehouse.\n",
    "For new incremental data or second and subsequent runs, we will run this file which makes sure that if there is a new field in the dimensions then we will add that and make appropriate addition to the surrogate keys as well, including changes to the data warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def add_scd2_columns(df):\n",
    "    df['start_date'] = datetime.now()\n",
    "    df['end_date'] = pd.to_datetime('2262-04-11')\n",
    "    df['active_flag'] = 'Y'\n",
    "    return df\n",
    "\n",
    "new_accounts_df = pd.read_excel(\"Processed_file\\Cleaned_data\\cleaned_accounts.xlsx\", sheet_name='Sheet1')\n",
    "new_deposits_df = pd.read_excel(\"Processed_file\\Cleaned_data\\cleaned_deposits.xlsx\", sheet_name='Sheet1')\n",
    "new_loans_df = pd.read_excel(\"Processed_file\\Cleaned_data\\cleaned_loans.xlsx\", sheet_name='Sheet1')\n",
    "\n",
    "\n",
    "new_accounts_df.rename(columns={'amount': 'account_amount'}, inplace=True)\n",
    "new_deposits_df.rename(columns={'amount': 'deposit_amount'}, inplace=True)\n",
    "new_loans_df.rename(columns={'amount': 'loan_amount'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n",
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n",
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n",
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n",
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n",
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n",
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n",
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n",
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n",
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n"
     ]
    }
   ],
   "source": [
    "import pandas_gbq\n",
    "from google.oauth2 import service_account\n",
    "import pandas as pd\n",
    "# Existing Data\n",
    "\n",
    "\n",
    "# Replace these with your actual paths and project information\n",
    "key_path = \"C:/Users/epranei/Downloads/calm-cove-423918-t0-ce8d5f6922f1.json\"\n",
    "project_id = 'calm-cove-423918-t0'\n",
    "dataset_id = 'Advisense'\n",
    "\n",
    "# Authenticate with Google Cloud\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "# Function to load table from BigQuery into a DataFrame\n",
    "def load_table_from_bigquery(table_name):\n",
    "    query = f\"SELECT * FROM `{project_id}.{dataset_id}.{table_name}`\"\n",
    "    return pandas_gbq.read_gbq(query, project_id=project_id, credentials=credentials)\n",
    "\n",
    "# Load the dimension tables\n",
    "existing_customers_df = load_table_from_bigquery('unique_customers')\n",
    "existing_countries_df = load_table_from_bigquery('unique_countries')\n",
    "existing_currencies_df = load_table_from_bigquery('unique_currencies')\n",
    "existing_account_names_df = load_table_from_bigquery('unique_account_names')\n",
    "existing_date_dimension_df = load_table_from_bigquery('date_dimension')\n",
    "existing_loan_type_df = load_table_from_bigquery('loan_type')\n",
    "existing_deposit_type_df = load_table_from_bigquery('deposit_type')\n",
    "\n",
    "# Load the fact tables\n",
    "existing_fact_deposits_df = load_table_from_bigquery('fact_deposits')\n",
    "existing_fact_accounts_df = load_table_from_bigquery('fact_accounts')\n",
    "existing_fact_loans_df = load_table_from_bigquery('fact_loans')\n",
    "\n",
    "# Now you can use the dataframes as needed\n",
    "# print(existing_customers_df.head())\n",
    "# print(existing_fact_loans_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Dimension Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update new Customer Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new customers to add.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Replace these with your actual paths and project information\n",
    "key_path = \"C:/Users/epranei/Downloads/calm-cove-423918-t0-ce8d5f6922f1.json\"\n",
    "project_id = 'calm-cove-423918-t0'\n",
    "dataset_id = 'Advisense'\n",
    "table_name = 'unique_customers'\n",
    "\n",
    "# Authenticate with Google Cloud\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "\n",
    "# Identify unique customers from new deposits and loans\n",
    "unique_new_customers_deposits = new_deposits_df[['customer', 'customer_type']].drop_duplicates()\n",
    "unique_new_customers_loans = new_loans_df[['customer', 'customer_type']].drop_duplicates()\n",
    "\n",
    "# Combine the new unique customers and remove duplicates\n",
    "all_unique_new_customers = pd.concat([unique_new_customers_deposits, unique_new_customers_loans]).drop_duplicates()\n",
    "\n",
    "# Find new customers that are not already in the existing customers table\n",
    "new_customers = pd.merge(all_unique_new_customers, existing_customers_df[['customer', 'customer_type']],\n",
    "                         on=['customer', 'customer_type'], how='left', indicator=True)\n",
    "new_customers = new_customers[new_customers['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "# Assign new surrogate keys to the new customers and SCD Type 2 columns\n",
    "if not new_customers.empty:\n",
    "    max_existing_key = existing_customers_df['customer_key'].max()\n",
    "    new_customers['customer_key'] = range(max_existing_key + 1, max_existing_key + 1 + len(new_customers))\n",
    "    new_customers = add_scd2_columns(new_customers)\n",
    "\n",
    "    # Upload new customers to BigQuery\n",
    "    table_id = f'{project_id}.{dataset_id}.{table_name}'\n",
    "    new_customers.to_gbq(table_id, project_id=project_id, if_exists='append', credentials=credentials)\n",
    "\n",
    "    print(\"New customers have been added to BigQuery.\")\n",
    "else:\n",
    "    print(\"No new customers to add.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update new Country Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new countries to add.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Replace these with your actual paths and project information\n",
    "key_path = \"C:/Users/epranei/Downloads/calm-cove-423918-t0-ce8d5f6922f1.json\"\n",
    "project_id = 'calm-cove-423918-t0'\n",
    "dataset_id = 'Advisense'\n",
    "table_name = 'unique_countries'\n",
    "\n",
    "# Authenticate with Google Cloud\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "\n",
    "\n",
    "# Identify unique countries from new deposits and loans\n",
    "unique_countries_deposits = new_deposits_df['country'].unique()\n",
    "unique_countries_loans = new_loans_df['country'].unique()\n",
    "\n",
    "# Combine the unique countries and remove duplicates\n",
    "all_unique_countries = np.unique(np.concatenate((unique_countries_deposits, unique_countries_loans)))\n",
    "\n",
    "# Create a DataFrame for unique new countries\n",
    "unique_countries_df = pd.DataFrame(all_unique_countries, columns=['country'])\n",
    "\n",
    "# Load existing countries from BigQuery\n",
    "existing_countries_df = pandas_gbq.read_gbq(f\"SELECT * FROM `{project_id}.{dataset_id}.{table_name}`\", project_id=project_id, credentials=credentials)\n",
    "\n",
    "# Find new countries that are not already in the existing countries table\n",
    "new_countries = pd.merge(unique_countries_df, existing_countries_df[['country']], on='country', how='left', indicator=True)\n",
    "new_countries = new_countries[new_countries['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "# Assign new surrogate keys to the new countries\n",
    "if not new_countries.empty:\n",
    "    max_existing_key = existing_countries_df['country_key'].max() if not existing_countries_df.empty else 0\n",
    "    new_countries['country_key'] = range(max_existing_key + 1, max_existing_key + 1 + len(new_countries))\n",
    "\n",
    "    new_countries = add_scd2_columns(new_countries)\n",
    "\n",
    "    # Upload new countries to BigQuery\n",
    "    new_countries.to_gbq(f'{dataset_id}.{table_name}', project_id=project_id, if_exists='append', credentials=credentials)\n",
    "\n",
    "    print(\"New countries have been added to BigQuery.\")\n",
    "else:\n",
    "    print(\"No new countries to add.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update new Currency Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new currencies to add.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Replace these with your actual paths and project information\n",
    "key_path = \"C:/Users/epranei/Downloads/calm-cove-423918-t0-ce8d5f6922f1.json\"\n",
    "project_id = 'calm-cove-423918-t0'\n",
    "dataset_id = 'Advisense'\n",
    "table_name = 'unique_currencies'\n",
    "\n",
    "# Authenticate with Google Cloud\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "# Identify unique currencies from new deposits and loans\n",
    "unique_currencies_deposits = new_deposits_df['currency'].unique()\n",
    "unique_currencies_loans = new_loans_df['currency'].unique()\n",
    "\n",
    "# Combine the unique currencies and remove duplicates\n",
    "all_unique_currencies = np.unique(np.concatenate((unique_currencies_deposits, unique_currencies_loans)))\n",
    "\n",
    "# Create a DataFrame for unique new currencies\n",
    "unique_currencies_df = pd.DataFrame(all_unique_currencies, columns=['currency'])\n",
    "\n",
    "# Load existing currencies from BigQuery\n",
    "existing_currencies_df = pandas_gbq.read_gbq(f\"SELECT * FROM `{project_id}.{dataset_id}.{table_name}`\", project_id=project_id, credentials=credentials)\n",
    "\n",
    "# Find new currencies that are not already in the existing currencies table\n",
    "new_currencies = pd.merge(unique_currencies_df, existing_currencies_df[['currency']], on='currency', how='left', indicator=True)\n",
    "new_currencies = new_currencies[new_currencies['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "# Assign new surrogate keys to the new currencies\n",
    "if not new_currencies.empty:\n",
    "    max_existing_key = existing_currencies_df['currency_key'].max() if not existing_currencies_df.empty else 0\n",
    "    new_currencies['currency_key'] = range(max_existing_key + 1, max_existing_key + 1 + len(new_currencies))\n",
    "\n",
    "    new_currencies = add_scd2_columns(new_currencies)\n",
    "    # Append new currencies to BigQuery\n",
    "    new_currencies.to_gbq(f'{dataset_id}.{table_name}', project_id=project_id, if_exists='append', credentials=credentials)\n",
    "\n",
    "    print(\"New currencies have been added to BigQuery.\")\n",
    "else:\n",
    "    print(\"No new currencies to add.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update new Accounts_name Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new account names to add.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Replace these with your actual paths and project information\n",
    "key_path = \"C:/Users/epranei/Downloads/calm-cove-423918-t0-ce8d5f6922f1.json\"\n",
    "project_id = 'calm-cove-423918-t0'\n",
    "dataset_id = 'Advisense'\n",
    "table_name = 'unique_account_names'\n",
    "\n",
    "# Authenticate with Google Cloud\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "\n",
    "# Identify unique account names from new accounts\n",
    "unique_account_names = pd.DataFrame(new_accounts_df['account_name'].unique(), columns=['account_name'])\n",
    "\n",
    "# Load existing account names from BigQuery\n",
    "existing_account_names_df = pandas_gbq.read_gbq(f\"SELECT * FROM `{project_id}.{dataset_id}.{table_name}`\", project_id=project_id, credentials=credentials)\n",
    "\n",
    "# Find new account names that are not already in the existing account names table\n",
    "new_account_names = pd.merge(unique_account_names, existing_account_names_df[['account_name']],\n",
    "                             on='account_name', how='left', indicator=True)\n",
    "new_account_names = new_account_names[new_account_names['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "# Assign new surrogate keys to the new account names\n",
    "if not new_account_names.empty:\n",
    "    max_existing_key = existing_account_names_df['account_name_key'].max() if not existing_account_names_df.empty else 0\n",
    "    new_account_names['account_name_key'] = range(max_existing_key + 1, max_existing_key + 1 + len(new_account_names))\n",
    "\n",
    "    # Use the add_scd2_columns function\n",
    "    new_account_names = add_scd2_columns(new_account_names)\n",
    "\n",
    "    # Append new account names to BigQuery\n",
    "    new_account_names.to_gbq(f'{dataset_id}.{table_name}', project_id=project_id, if_exists='append', credentials=credentials)\n",
    "\n",
    "    print(\"New account names have been added to BigQuery.\")\n",
    "else:\n",
    "    print(\"No new account names to add.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update new Loan Type Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new loan types to add.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Replace these with your actual paths and project information\n",
    "key_path = \"C:/Users/epranei/Downloads/calm-cove-423918-t0-ce8d5f6922f1.json\"\n",
    "project_id = 'calm-cove-423918-t0'\n",
    "dataset_id = 'Advisense'\n",
    "table_name = 'loan_type'\n",
    "\n",
    "# Authenticate with Google Cloud\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "# Identify unique loan types from new loans\n",
    "unique_new_loan_types = new_loans_df['loan_type'].unique()\n",
    "\n",
    "# Create a DataFrame for unique new loan types\n",
    "new_loan_types_df = pd.DataFrame(unique_new_loan_types, columns=['loan_type'])\n",
    "\n",
    "# Load existing loan types from BigQuery\n",
    "existing_loan_type_df = pandas_gbq.read_gbq(f\"SELECT * FROM `{project_id}.{dataset_id}.{table_name}`\", project_id=project_id, credentials=credentials)\n",
    "\n",
    "# Find new loan types that are not already in the existing loan types table\n",
    "new_loan_types = pd.merge(new_loan_types_df, existing_loan_type_df[['loan_type']],\n",
    "                          on='loan_type', how='left', indicator=True)\n",
    "new_loan_types = new_loan_types[new_loan_types['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "# Assign new surrogate keys to the new loan types\n",
    "if not new_loan_types.empty:\n",
    "    max_existing_key = existing_loan_type_df['loan_type_key'].max() if not existing_loan_type_df.empty else 0\n",
    "    new_loan_types['loan_type_key'] = range(max_existing_key + 1, max_existing_key + 1 + len(new_loan_types))\n",
    "\n",
    "    # Use the add_scd2_columns function\n",
    "    new_loan_types = add_scd2_columns(new_loan_types)\n",
    "\n",
    "    # Append new loan types to BigQuery\n",
    "    new_loan_types.to_gbq(f'{dataset_id}.{table_name}', project_id=project_id, if_exists='append', credentials=credentials)\n",
    "\n",
    "    print(\"New loan types have been added to BigQuery.\")\n",
    "else:\n",
    "    print(\"No new loan types to add.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update new Deposit Type Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new deposit types to add.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Replace these with your actual paths and project information\n",
    "key_path = \"C:/Users/epranei/Downloads/calm-cove-423918-t0-ce8d5f6922f1.json\"\n",
    "project_id = 'calm-cove-423918-t0'\n",
    "dataset_id = 'Advisense'\n",
    "table_name = 'deposit_type'\n",
    "\n",
    "# Authenticate with Google Cloud\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "# Identify unique deposit types from new deposits\n",
    "unique_new_deposit_types = new_deposits_df['deposit_type'].unique()\n",
    "\n",
    "# Create a DataFrame for unique new deposit types\n",
    "new_deposit_types_df = pd.DataFrame(unique_new_deposit_types, columns=['deposit_type'])\n",
    "\n",
    "# Load existing deposit types from BigQuery\n",
    "existing_deposit_type_df = pandas_gbq.read_gbq(f\"SELECT * FROM `{project_id}.{dataset_id}.{table_name}`\", project_id=project_id, credentials=credentials)\n",
    "\n",
    "# Find new deposit types that are not already in the existing deposit types table\n",
    "new_deposit_types = pd.merge(new_deposit_types_df, existing_deposit_type_df[['deposit_type']],\n",
    "                             on='deposit_type', how='left', indicator=True)\n",
    "new_deposit_types = new_deposit_types[new_deposit_types['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "# Assign new surrogate keys to the new deposit types\n",
    "if not new_deposit_types.empty:\n",
    "    max_existing_key = existing_deposit_type_df['deposit_type_key'].max() if not existing_deposit_type_df.empty else 0\n",
    "    new_deposit_types['deposit_type_key'] = range(max_existing_key + 1, max_existing_key + 1 + len(new_deposit_types))\n",
    "\n",
    "    # Use the add_scd2_columns function\n",
    "    new_deposit_types = add_scd2_columns(new_deposit_types)\n",
    "\n",
    "    # Append new deposit types to BigQuery\n",
    "    new_deposit_types.to_gbq(f'{dataset_id}.{table_name}', project_id=project_id, if_exists='append', credentials=credentials)\n",
    "\n",
    "    print(\"New deposit types have been added to BigQuery.\")\n",
    "else:\n",
    "    print(\"No new deposit types to add.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maping Dimensions to Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fact_accounts_df = new_accounts_df.copy()\n",
    "new_fact_deposits_df = new_deposits_df.copy()\n",
    "new_fact_loans_df = new_loans_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maping date_dimension to Fact_tables\n",
    "1. Map all the dates with the key \n",
    "2. Drop the original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map dates to date keys\n",
    "def map_date_to_key(df, date_column):\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df[date_column + '_key'] = df[date_column].dt.strftime('%Y%m%d').astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Add date keys to fact_accounts\n",
    "fact_accounts = map_date_to_key(new_fact_accounts_df, 'reference_date')\n",
    "\n",
    "# Add date keys to fact_deposits\n",
    "fact_deposits = map_date_to_key(new_fact_deposits_df, 'start_date')\n",
    "fact_deposits = map_date_to_key(new_fact_deposits_df, 'maturity_date')\n",
    "fact_deposits = map_date_to_key(new_fact_deposits_df, 'reference_date')\n",
    "\n",
    "# Add date keys to fact_loans\n",
    "fact_loans = map_date_to_key(new_fact_loans_df, 'start_date')\n",
    "fact_loans = map_date_to_key(new_fact_loans_df, 'maturity_date')\n",
    "fact_loans = map_date_to_key(new_fact_loans_df, 'reference_date')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fact_deposits_df.drop(columns = ['start_date','maturity_date','reference_date'], inplace=True)\n",
    "new_fact_loans_df.drop(columns = ['start_date','maturity_date','reference_date'], inplace=True)\n",
    "new_fact_accounts_df.drop(columns=['reference_date'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have all the updated dimension tables in the data warehouse which now includes additional dimensions if included in the incremental data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n",
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n",
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n",
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n",
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n",
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n"
     ]
    }
   ],
   "source": [
    "updated_customers_df = load_table_from_bigquery('unique_customers')\n",
    "updated_countries_df = load_table_from_bigquery('unique_countries')\n",
    "updated_currencies_df = load_table_from_bigquery('unique_currencies')\n",
    "updated_account_names_df = load_table_from_bigquery('unique_account_names')\n",
    "updated_date_dimension_df = load_table_from_bigquery('date_dimension')\n",
    "updated_loan_type_df = load_table_from_bigquery('loan_type')\n",
    "updated_deposit_type_df = load_table_from_bigquery('deposit_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maping updated_cutomer_dimension to fact_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_customer_to_key(df, unique_customers_df):\n",
    "    df = df.merge(updated_customers_df[['customer', 'customer_key']], on='customer', how='left')\n",
    "    # df.drop(columns=['customer'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Add customer_key to fact_deposits and drop the original customer column\n",
    "new_fact_deposits_df = map_customer_to_key(fact_deposits,updated_customers_df )\n",
    "\n",
    "# Add customer_key to fact_loans and drop the original customer column\n",
    "new_fact_loans_df = map_customer_to_key(fact_loans,updated_customers_df )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fact_deposits_df.drop(columns=['customer','customer_type'], inplace=True)\n",
    "new_fact_loans_df.drop(columns=['customer','customer_type'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maping updated_account_name dimensions to fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_account_name_to_key(df, account_name_df):\n",
    "    df = df.merge(updated_account_names_df, on='account_name', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "new_fact_accounts_df = map_account_name_to_key(new_fact_accounts_df, updated_account_names_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fact_accounts_df.drop(columns=['account_name'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maping Updated_Currency dimension to  fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_currency_to_key(df, unique_currencies_df):\n",
    "    df = df.merge(updated_currencies_df, on='currency', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "# Add currency_key to fact_deposits and drop the original currency column\n",
    "new_fact_deposits_df = map_currency_to_key(new_fact_deposits_df, updated_currencies_df)\n",
    "\n",
    "# Add currency_key to fact_loans and drop the original currency column\n",
    "new_fact_loans_df = map_currency_to_key(new_fact_loans_df, updated_currencies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fact_deposits_df.drop(columns=['currency'], inplace= True)\n",
    "new_fact_loans_df.drop(columns=['currency'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Updated Country dimension to Fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_country_to_key(df, unique_countries_df):\n",
    "    df = df.merge(updated_countries_df, on='country', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Add country_key to fact_deposits and drop the original country column\n",
    "new_fact_deposits_df = map_country_to_key(new_fact_deposits_df, updated_countries_df)\n",
    "\n",
    "# Add country_key to fact_loans and drop the original country column\n",
    "new_fact_loans_df = map_country_to_key(new_fact_loans_df, updated_countries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fact_deposits_df.drop(columns=['country'], inplace= True)\n",
    "new_fact_loans_df.drop(columns=['country'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Updated Deposit_type dimensions to  Fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_deposit_type_to_key(df, deposit_type_df):\n",
    "    df = df.merge(updated_deposit_type_df, on='deposit_type', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "# Add deposit_type_key to fact_deposits and drop the original deposit_type column\n",
    "new_fact_deposits_df = map_deposit_type_to_key(new_fact_deposits_df, updated_deposit_type_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fact_deposits_df.drop(columns=['deposit_type'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Updated Loan_type dimensions to Fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_loan_type_to_key(df, loan_type_df):\n",
    "    df = df.merge(updated_loan_type_df, on='loan_type', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Add loan_type_key to fact_loans and drop the original loan_type column\n",
    "new_fact_loans_df = map_loan_type_to_key(new_fact_loans_df, updated_loan_type_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fact_loans_df.drop(columns=['loan_type'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Final new incremental data for Loading in data warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Persisted maximum surrogate keys to maintain primary key in fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Big_Data_Engineering\\Advisence\\Project\\advicense\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:2365: UserWarning: A progress bar was requested, but there was an error loading the tqdm library. Please install tqdm to use the progress bar functionality.\n",
      "  record_batch = self.to_arrow(\n"
     ]
    }
   ],
   "source": [
    "max_keys_df = load_table_from_bigquery('max_keys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final new Fact accounts data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_accounts_key = max_keys_df[max_keys_df['table_name'] == 'fact_accounts']['max_account_surr_primarykey'].iloc[0]\n",
    "\n",
    "# Increment the surrogate primary keys for the new records\n",
    "new_fact_accounts_df['account_surr_primarykey'] = range(int(max_accounts_key) + 1, int(max_accounts_key) + 1 + len(new_fact_accounts_df))\n",
    "\n",
    "new_fact_accounts_df = new_fact_accounts_df[['account_number', 'account_amount','account_surr_primarykey','account_type', 'ingest_date_time', 'reference_date_key', 'account_name_key']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final new Fact loans data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_loans_key = max_keys_df[max_keys_df['table_name'] == 'fact_loans']['max_surr_primarykey'].iloc[0]\n",
    "\n",
    "new_fact_loans_df['loans_surr_primarykey'] =  range(int(max_loans_key) + 1, int(max_loans_key) + 1 + len(new_fact_loans_df))\n",
    "new_fact_loans_df = new_fact_loans_df[['loan_amount','exchange_rate','ingest_date_time','loans_surr_primarykey','start_date_key','maturity_date_key','reference_date_key','customer_key','currency_key','country_key','loan_type_key']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final new Facts Deposits Data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_deposits_key = max_keys_df[max_keys_df['table_name'] == 'fact_deposits']['max_deposits_surr_primarykey'].iloc[0]\n",
    "\n",
    "new_fact_deposits_df['deposits_surr_primarykey'] = range(int(max_deposits_key) + 1, int(max_deposits_key) + 1 + len(new_fact_deposits_df))\n",
    "new_fact_deposits_df = new_fact_deposits_df[['deposit_amount','exchange_rate','ingest_date_time','deposits_surr_primarykey','start_date_key','maturity_date_key','reference_date_key','customer_key','currency_key','country_key','deposit_type_key']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append Fact tables to Data warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_29700\\299076875.py:16: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists=if_exists, credentials=credentials)\n",
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_29700\\299076875.py:16: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists=if_exists, credentials=credentials)\n",
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_29700\\299076875.py:16: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists=if_exists, credentials=credentials)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded to BigQuery successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Replace these with your actual paths and project information\n",
    "key_path = \"C:/Users/epranei/Downloads/calm-cove-423918-t0-ce8d5f6922f1.json\"\n",
    "project_id = 'calm-cove-423918-t0'\n",
    "dataset_id = 'Advisense'\n",
    "\n",
    "# Authenticate with Google Cloud\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "# Define a function to upload a DataFrame to BigQuery\n",
    "def upload_to_bigquery(df, table_name, if_exists='append'):\n",
    "    table_id = f'{dataset_id}.{table_name}'\n",
    "    df.to_gbq(table_id, project_id=project_id, if_exists=if_exists, credentials=credentials)\n",
    "\n",
    "# Append new_fact_deposits_df to the BigQuery table\n",
    "upload_to_bigquery(new_fact_deposits_df, 'fact_deposits')\n",
    "\n",
    "# Append new_fact_loans_df to the BigQuery table\n",
    "upload_to_bigquery(new_fact_loans_df, 'fact_loans')\n",
    "\n",
    "# Append new_fact_accounts_df to the BigQuery table\n",
    "upload_to_bigquery(new_fact_accounts_df, 'fact_accounts')\n",
    "\n",
    "print(\"Data uploaded to BigQuery successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the max surrogate key and Persisting the same in the data warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epranei\\AppData\\Local\\Temp\\ipykernel_29700\\1210910599.py:39: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  df.to_gbq(table_id, project_id=project_id, if_exists=if_exists, credentials=credentials)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max keys data uploaded to BigQuery successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pandas_gbq\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Create the Data_warehouse folder if it doesn't exist (optional for local operations)\n",
    "os.makedirs('Data_warehouse', exist_ok=True)\n",
    "\n",
    "# Replace these with your actual paths and project information\n",
    "key_path = \"C:/Users/epranei/Downloads/calm-cove-423918-t0-ce8d5f6922f1.json\"\n",
    "project_id = 'calm-cove-423918-t0'\n",
    "dataset_id = 'Advisense'\n",
    "table_name = 'max_keys'\n",
    "\n",
    "# Authenticate with Google Cloud\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "\n",
    "# Calculate max surrogate keys from new data\n",
    "max_deposits_key = new_fact_deposits_df['deposits_surr_primarykey'].max()\n",
    "max_accounts_key = new_fact_accounts_df['account_surr_primarykey'].max()\n",
    "max_loans_key = new_fact_loans_df['loans_surr_primarykey'].max()\n",
    "\n",
    "# Create DataFrame for max keys data\n",
    "max_keys_data = [\n",
    "    {\"table_name\": \"fact_deposits\", \"max_deposits_surr_primarykey\": max_deposits_key},\n",
    "    {\"table_name\": \"fact_accounts\", \"max_account_surr_primarykey\": max_accounts_key},\n",
    "    {\"table_name\": \"fact_loans\", \"max_surr_primarykey\": max_loans_key}\n",
    "]\n",
    "max_keys_df = pd.DataFrame(max_keys_data)\n",
    "\n",
    "# Fill NaN values and convert to int\n",
    "max_keys_df['max_deposits_surr_primarykey'] = max_keys_df['max_deposits_surr_primarykey'].fillna(0).astype(int)\n",
    "max_keys_df['max_account_surr_primarykey'] = max_keys_df['max_account_surr_primarykey'].fillna(0).astype(int)\n",
    "max_keys_df['max_surr_primarykey'] = max_keys_df['max_surr_primarykey'].fillna(0).astype(int)\n",
    "\n",
    "# Define a function to upload a DataFrame to BigQuery\n",
    "def upload_to_bigquery(df, table_name, if_exists='replace'):\n",
    "    table_id = f'{dataset_id}.{table_name}'\n",
    "    df.to_gbq(table_id, project_id=project_id, if_exists=if_exists, credentials=credentials)\n",
    "\n",
    "# Upload max_keys_df to BigQuery\n",
    "upload_to_bigquery(max_keys_df, table_name)\n",
    "\n",
    "print(\"Max keys data uploaded to BigQuery successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_name</th>\n",
       "      <th>max_deposits_surr_primarykey</th>\n",
       "      <th>max_account_surr_primarykey</th>\n",
       "      <th>max_surr_primarykey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fact_deposits</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fact_accounts</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fact_loans</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      table_name  max_deposits_surr_primarykey  max_account_surr_primarykey  \\\n",
       "0  fact_deposits                           180                            0   \n",
       "1  fact_accounts                             0                           32   \n",
       "2     fact_loans                             0                            0   \n",
       "\n",
       "   max_surr_primarykey  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                  600  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_keys_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advicense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
